{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM8MRkf8Dr94"
      },
      "source": [
        "## Describe your model -> fine-tuned GPT-3.5\n",
        "By Matt Shumer (https://twitter.com/mattshumer_)\n",
        "\n",
        "The goal of this notebook is to experiment with a new way to make it very easy to build a task-specific model for your use-case.\n",
        "\n",
        "First, use the best GPU available (go to Runtime -> change runtime type)\n",
        "\n",
        "To create your model, just go to the first code cell, and describe the model you want to build in the prompt. Be descriptive and clear.\n",
        "\n",
        "Select a temperature (high=creative, low=precise), and the number of training examples to generate to train the model. From there, just run all the cells.\n",
        "\n",
        "You can change the model you want to fine-tune by changing `model_name` in the `Define Hyperparameters` cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Way3_PuPpIuE"
      },
      "source": [
        "#Data generation step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY-3DvlIpVSl"
      },
      "source": [
        "Write your prompt here. Make it as descriptive as possible!\n",
        "\n",
        "Then, choose the temperature (between 0 and 1) to use when generating data. Lower values are great for precise tasks, like writing code, whereas larger values are better for creative tasks, like writing stories.\n",
        "\n",
        "Finally, choose how many examples you want to generate. The more you generate, a) the longer it takes and b) the more expensive data generation will be. But generally, more examples will lead to a higher-quality model. 100 is usually the minimum to start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "R7WKZyxtpUPS"
      },
      "outputs": [],
      "source": [
        "prompt = \"A model that acts as a specialized Robotics SDK Translator. It will be given a high-level, natural language command from a user (e.g., 'Go to the charging dock and then report your battery level'). It must respond with a syntactically correct, logical sequence of Python code, specifically using ROS2 functions and Unitree SDK commands for navigation and system-checking. The response must be only the code, formatted as a Python list of command strings.\"\n",
        "temperature = 0.2\n",
        "number_of_examples = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1snNou5PrIci"
      },
      "source": [
        "Run this to generate the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rdsd82ngpHCG",
        "outputId": "9dc1516a-2803-4201-eedd-9fd6a0e97004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating example 0\n",
            "Generating example 1\n",
            "Generating example 1\n",
            "Generating example 2\n",
            "Generating example 2\n",
            "Generating example 3\n",
            "Generating example 3\n",
            "Generating example 4\n",
            "Generating example 4\n",
            "Generating example 5\n",
            "Generating example 5\n",
            "Generating example 6\n",
            "Generating example 6\n",
            "Generating example 7\n",
            "Generating example 7\n",
            "Generating example 8\n",
            "Generating example 8\n",
            "Generating example 9\n",
            "Generating example 9\n",
            "['prompt\\n-----------\\nMove to the coordinates (3, 4) and then report your current position.\\n-----------\\nresponse\\n-----------\\n[\\n\"import rclpy\",\\n\"from unitree_legged_msgs.msg import Position\",\\n\"from geometry_msgs.msg import Point\",\\n\"rclpy.init()\",\\n\"node = rclpy.create_node(\\'move_and_report\\')\",\\n\"pub = node.create_publisher(Position, \\'/unitree_legged_control/position\\', 10)\",\\n\"point = Point()\",\\n\"point.x = 3\",\\n\"point.y = 4\",\\n\"msg = Position()\",\\n\"msg.pose.position = point\",\\n\"pub.publish(msg)\",\\n\"rclpy.spin(node)\",\\n\"node.destroy_node()\",\\n\"rclpy.shutdown()\"\\n]\\n-----------', 'prompt\\n-----------\\nRotate 90 degrees to the right and then report your current orientation.\\n-----------\\nresponse\\n-----------\\n[\\n\"import rclpy\",\\n\"from unitree_legged_msgs.msg import Orientation\",\\n\"from geometry_msgs.msg import Quaternion\",\\n\"rclpy.init()\",\\n\"node = rclpy.create_node(\\'rotate_and_report\\')\",\\n\"pub = node.create_publisher(Orientation, \\'/unitree_legged_control/orientation\\', 10)\",\\n\"quaternion = Quaternion()\",\\n\"quaternion.z = 0.7071\",\\n\"quaternion.w = 0.7071\",\\n\"msg = Orientation()\",\\n\"msg.pose.orientation = quaternion\",\\n\"pub.publish(msg)\",\\n\"rclpy.spin(node)\",\\n\"node.destroy_node()\",\\n\"rclpy.shutdown()\"\\n]\\n-----------', 'prompt\\n-----------\\nWalk forward for 5 seconds, then stop and report your current position.\\n-----------\\nresponse\\n-----------\\n[\\n\"import rclpy\",\\n\"from unitree_legged_msgs.msg import Position\",\\n\"from geometry_msgs.msg import Point\",\\n\"import time\",\\n\"rclpy.init()\",\\n\"node = rclpy.create_node(\\'walk_and_report\\')\",\\n\"pub = node.create_publisher(Position, \\'/unitree_legged_control/position\\', 10)\",\\n\"point = Point()\",\\n\"point.x = 5\",\\n\"point.y = 0\",\\n\"msg = Position()\",\\n\"msg.pose.position = point\",\\n\"pub.publish(msg)\",\\n\"time.sleep(5)\",\\n\"point.x = 0\",\\n\"point.y = 0\",\\n\"msg.pose.position = point\",\\n\"pub.publish(msg)\",\\n\"rclpy.spin(node)\",\\n\"node.destroy_node()\",\\n\"rclpy.shutdown()\"\\n]\\n-----------', 'prompt\\n-----------\\nJump to a height of 1 meter, then report your current height.\\n-----------\\nresponse\\n-----------\\n[\\n\"import rclpy\",\\n\"from unitree_legged_msgs.msg import Position\",\\n\"from geometry_msgs.msg import Point\",\\n\"rclpy.init()\",\\n\"node = rclpy.create_node(\\'jump_and_report\\')\",\\n\"pub = node.create_publisher(Position, \\'/unitree_legged_control/position\\', 10)\",\\n\"point = Point()\",\\n\"point.z = 1\",\\n\"msg = Position()\",\\n\"msg.pose.position = point\",\\n\"pub.publish(msg)\",\\n\"rclpy.spin(node)\",\\n\"node.destroy_node()\",\\n\"rclpy.shutdown()\"\\n]\\n-----------', 'prompt\\n-----------\\nTurn on your lights, then report your current light status.\\n-----------\\nresponse\\n-----------\\n[\\n\"import rclpy\",\\n\"from unitree_legged_msgs.msg import Light\",\\n\"rclpy.init()\",\\n\"node = rclpy.create_node(\\'light_on_and_report\\')\",\\n\"pub = node.create_publisher(Light, \\'/unitree_legged_control/light\\', 10)\",\\n\"msg = Light()\",\\n\"msg.status = True\",\\n\"pub.publish(msg)\",\\n\"rclpy.spin(node)\",\\n\"node.destroy_node()\",\\n\"rclpy.shutdown()\"\\n]\\n-----------', 'prompt\\n-----------\\nMove to the charging dock, then report your current battery level.\\n-----------\\nresponse\\n-----------\\n[\\n\"import rclpy\",\\n\"from unitree_legged_msgs.msg import Position, BatteryStatus\",\\n\"from geometry_msgs.msg import Point\",\\n\"rclpy.init()\",\\n\"node = rclpy.create_node(\\'move_charge_and_report\\')\",\\n\"pub_move = node.create_publisher(Position, \\'/unitree_legged_control/position\\', 10)\",\\n\"pub_battery = node.create_publisher(BatteryStatus, \\'/unitree_legged_control/battery_status\\', 10)\",\\n\"point = Point()\",\\n\"point.x = 0\",\\n\"point.y = 0\",\\n\"msg_move = Position()\",\\n\"msg_move.pose.position = point\",\\n\"pub_move.publish(msg_move)\",\\n\"msg_battery = BatteryStatus()\",\\n\"pub_battery.publish(msg_battery)\",\\n\"rclpy.spin(node)\",\\n\"node.destroy_node()\",\\n\"rclpy.shutdown()\"\\n]\\n-----------', 'prompt\\n-----------\\nPerform a 360-degree spin, then report your current orientation.\\n-----------\\nresponse\\n-----------\\n[\\n\"import rclpy\",\\n\"from unitree_legged_msgs.msg import Orientation\",\\n\"from geometry_msgs.msg import Quaternion\",\\n\"rclpy.init()\",\\n\"node = rclpy.create_node(\\'spin_and_report\\')\",\\n\"pub = node.create_publisher(Orientation, \\'/unitree_legged_control/orientation\\', 10)\",\\n\"quaternion = Quaternion()\",\\n\"quaternion.z = 1\",\\n\"quaternion.w = 0\",\\n\"msg = Orientation()\",\\n\"msg.pose.orientation = quaternion\",\\n\"pub.publish(msg)\",\\n\"rclpy.spin(node)\",\\n\"node.destroy_node()\",\\n\"rclpy.shutdown()\"\\n]\\n-----------', 'prompt\\n-----------\\nMove to the coordinates (10, 15), then report your current position and orientation.\\n-----------\\nresponse\\n-----------\\n[\\n\"import rclpy\",\\n\"from unitree_legged_msgs.msg import Position, Orientation\",\\n\"from geometry_msgs.msg import Point, Quaternion\",\\n\"rclpy.init()\",\\n\"node = rclpy.create_node(\\'move_and_report\\')\",\\n\"pub_pos = node.create_publisher(Position, \\'/unitree_legged_control/position\\', 10)\",\\n\"pub_ori = node.create_publisher(Orientation, \\'/unitree_legged_control/orientation\\', 10)\",\\n\"point = Point()\",\\n\"point.x = 10\",\\n\"point.y = 15\",\\n\"msg_pos = Position()\",\\n\"msg_pos.pose.position = point\",\\n\"pub_pos.publish(msg_pos)\",\\n\"quaternion = Quaternion()\",\\n\"quaternion.z = 0\",\\n\"quaternion.w = 1\",\\n\"msg_ori = Orientation()\",\\n\"msg_ori.pose.orientation = quaternion\",\\n\"pub_ori.publish(msg_ori)\",\\n\"rclpy.spin(node)\",\\n\"node.destroy_node()\",\\n\"rclpy.shutdown()\"\\n]\\n-----------', 'prompt\\n-----------\\nWalk forward for 10 seconds, then stop and report your current position and battery level.\\n-----------\\nresponse\\n-----------\\n[\\n\"import rclpy\",\\n\"from unitree_legged_msgs.msg import Position, BatteryStatus\",\\n\"from geometry_msgs.msg import Point\",\\n\"import time\",\\n\"rclpy.init()\",\\n\"node = rclpy.create_node(\\'walk_and_report\\')\",\\n\"pub_pos = node.create_publisher(Position, \\'/unitree_legged_control/position\\', 10)\",\\n\"pub_battery = node.create_publisher(BatteryStatus, \\'/unitree_legged_control/battery_status\\', 10)\",\\n\"point = Point()\",\\n\"point.x = 10\",\\n\"point.y = 0\",\\n\"msg_pos = Position()\",\\n\"msg_pos.pose.position = point\",\\n\"pub_pos.publish(msg_pos)\",\\n\"time.sleep(10)\",\\n\"point.x = 0\",\\n\"point.y = 0\",\\n\"msg_pos.pose.position = point\",\\n\"pub_pos.publish(msg_pos)\",\\n\"msg_battery = BatteryStatus()\",\\n\"pub_battery.publish(msg_battery)\",\\n\"rclpy.spin(node)\",\\n\"node.destroy_node()\",\\n\"rclpy.shutdown()\"\\n]\\n-----------', 'prompt\\n-----------\\nWalk backwards for 7 seconds, then stop and report your current position.\\n-----------\\nresponse\\n-----------\\n[\\n\"import rclpy\",\\n\"from unitree_legged_msgs.msg import Position\",\\n\"from geometry_msgs.msg import Point\",\\n\"import time\",\\n\"rclpy.init()\",\\n\"node = rclpy.create_node(\\'walk_back_and_report\\')\",\\n\"pub = node.create_publisher(Position, \\'/unitree_legged_control/position\\', 10)\",\\n\"point = Point()\",\\n\"point.x = -7\",\\n\"point.y = 0\",\\n\"msg = Position()\",\\n\"msg.pose.position = point\",\\n\"pub.publish(msg)\",\\n\"time.sleep(7)\",\\n\"point.x = 0\",\\n\"point.y = 0\",\\n\"msg.pose.position = point\",\\n\"pub.publish(msg)\",\\n\"rclpy.spin(node)\",\\n\"node.destroy_node()\",\\n\"rclpy.shutdown()\"\\n]\\n-----------']\n",
            "['prompt\\n-----------\\nMove to the coordinates (3, 4) and then report your current position.\\n-----------\\nresponse\\n-----------\\n[\\n\"import rclpy\",\\n\"from unitree_legged_msgs.msg import Position\",\\n\"from geometry_msgs.msg import Point\",\\n\"rclpy.init()\",\\n\"node = rclpy.create_node(\\'move_and_report\\')\",\\n\"pub = node.create_publisher(Position, \\'/unitree_legged_control/position\\', 10)\",\\n\"point = Point()\",\\n\"point.x = 3\",\\n\"point.y = 4\",\\n\"msg = Position()\",\\n\"msg.pose.position = point\",\\n\"pub.publish(msg)\",\\n\"rclpy.spin(node)\",\\n\"node.destroy_node()\",\\n\"rclpy.shutdown()\"\\n]\\n-----------', 'prompt\\n-----------\\nRotate 90 degrees to the right and then report your current orientation.\\n-----------\\nresponse\\n-----------\\n[\\n\"import rclpy\",\\n\"from unitree_legged_msgs.msg import Orientation\",\\n\"from geometry_msgs.msg import Quaternion\",\\n\"rclpy.init()\",\\n\"node = rclpy.create_node(\\'rotate_and_report\\')\",\\n\"pub = node.create_publisher(Orientation, \\'/unitree_legged_control/orientation\\', 10)\",\\n\"quaternion = Quaternion()\",\\n\"quaternion.z = 0.7071\",\\n\"quaternion.w = 0.7071\",\\n\"msg = Orientation()\",\\n\"msg.pose.orientation = quaternion\",\\n\"pub.publish(msg)\",\\n\"rclpy.spin(node)\",\\n\"node.destroy_node()\",\\n\"rclpy.shutdown()\"\\n]\\n-----------', 'prompt\\n-----------\\nWalk forward for 5 seconds, then stop and report your current position.\\n-----------\\nresponse\\n-----------\\n[\\n\"import rclpy\",\\n\"from unitree_legged_msgs.msg import Position\",\\n\"from geometry_msgs.msg import Point\",\\n\"import time\",\\n\"rclpy.init()\",\\n\"node = rclpy.create_node(\\'walk_and_report\\')\",\\n\"pub = node.create_publisher(Position, \\'/unitree_legged_control/position\\', 10)\",\\n\"point = Point()\",\\n\"point.x = 5\",\\n\"point.y = 0\",\\n\"msg = Position()\",\\n\"msg.pose.position = point\",\\n\"pub.publish(msg)\",\\n\"time.sleep(5)\",\\n\"point.x = 0\",\\n\"point.y = 0\",\\n\"msg.pose.position = point\",\\n\"pub.publish(msg)\",\\n\"rclpy.spin(node)\",\\n\"node.destroy_node()\",\\n\"rclpy.shutdown()\"\\n]\\n-----------', 'prompt\\n-----------\\nJump to a height of 1 meter, then report your current height.\\n-----------\\nresponse\\n-----------\\n[\\n\"import rclpy\",\\n\"from unitree_legged_msgs.msg import Position\",\\n\"from geometry_msgs.msg import Point\",\\n\"rclpy.init()\",\\n\"node = rclpy.create_node(\\'jump_and_report\\')\",\\n\"pub = node.create_publisher(Position, \\'/unitree_legged_control/position\\', 10)\",\\n\"point = Point()\",\\n\"point.z = 1\",\\n\"msg = Position()\",\\n\"msg.pose.position = point\",\\n\"pub.publish(msg)\",\\n\"rclpy.spin(node)\",\\n\"node.destroy_node()\",\\n\"rclpy.shutdown()\"\\n]\\n-----------', 'prompt\\n-----------\\nTurn on your lights, then report your current light status.\\n-----------\\nresponse\\n-----------\\n[\\n\"import rclpy\",\\n\"from unitree_legged_msgs.msg import Light\",\\n\"rclpy.init()\",\\n\"node = rclpy.create_node(\\'light_on_and_report\\')\",\\n\"pub = node.create_publisher(Light, \\'/unitree_legged_control/light\\', 10)\",\\n\"msg = Light()\",\\n\"msg.status = True\",\\n\"pub.publish(msg)\",\\n\"rclpy.spin(node)\",\\n\"node.destroy_node()\",\\n\"rclpy.shutdown()\"\\n]\\n-----------', 'prompt\\n-----------\\nMove to the charging dock, then report your current battery level.\\n-----------\\nresponse\\n-----------\\n[\\n\"import rclpy\",\\n\"from unitree_legged_msgs.msg import Position, BatteryStatus\",\\n\"from geometry_msgs.msg import Point\",\\n\"rclpy.init()\",\\n\"node = rclpy.create_node(\\'move_charge_and_report\\')\",\\n\"pub_move = node.create_publisher(Position, \\'/unitree_legged_control/position\\', 10)\",\\n\"pub_battery = node.create_publisher(BatteryStatus, \\'/unitree_legged_control/battery_status\\', 10)\",\\n\"point = Point()\",\\n\"point.x = 0\",\\n\"point.y = 0\",\\n\"msg_move = Position()\",\\n\"msg_move.pose.position = point\",\\n\"pub_move.publish(msg_move)\",\\n\"msg_battery = BatteryStatus()\",\\n\"pub_battery.publish(msg_battery)\",\\n\"rclpy.spin(node)\",\\n\"node.destroy_node()\",\\n\"rclpy.shutdown()\"\\n]\\n-----------', 'prompt\\n-----------\\nPerform a 360-degree spin, then report your current orientation.\\n-----------\\nresponse\\n-----------\\n[\\n\"import rclpy\",\\n\"from unitree_legged_msgs.msg import Orientation\",\\n\"from geometry_msgs.msg import Quaternion\",\\n\"rclpy.init()\",\\n\"node = rclpy.create_node(\\'spin_and_report\\')\",\\n\"pub = node.create_publisher(Orientation, \\'/unitree_legged_control/orientation\\', 10)\",\\n\"quaternion = Quaternion()\",\\n\"quaternion.z = 1\",\\n\"quaternion.w = 0\",\\n\"msg = Orientation()\",\\n\"msg.pose.orientation = quaternion\",\\n\"pub.publish(msg)\",\\n\"rclpy.spin(node)\",\\n\"node.destroy_node()\",\\n\"rclpy.shutdown()\"\\n]\\n-----------', 'prompt\\n-----------\\nMove to the coordinates (10, 15), then report your current position and orientation.\\n-----------\\nresponse\\n-----------\\n[\\n\"import rclpy\",\\n\"from unitree_legged_msgs.msg import Position, Orientation\",\\n\"from geometry_msgs.msg import Point, Quaternion\",\\n\"rclpy.init()\",\\n\"node = rclpy.create_node(\\'move_and_report\\')\",\\n\"pub_pos = node.create_publisher(Position, \\'/unitree_legged_control/position\\', 10)\",\\n\"pub_ori = node.create_publisher(Orientation, \\'/unitree_legged_control/orientation\\', 10)\",\\n\"point = Point()\",\\n\"point.x = 10\",\\n\"point.y = 15\",\\n\"msg_pos = Position()\",\\n\"msg_pos.pose.position = point\",\\n\"pub_pos.publish(msg_pos)\",\\n\"quaternion = Quaternion()\",\\n\"quaternion.z = 0\",\\n\"quaternion.w = 1\",\\n\"msg_ori = Orientation()\",\\n\"msg_ori.pose.orientation = quaternion\",\\n\"pub_ori.publish(msg_ori)\",\\n\"rclpy.spin(node)\",\\n\"node.destroy_node()\",\\n\"rclpy.shutdown()\"\\n]\\n-----------', 'prompt\\n-----------\\nWalk forward for 10 seconds, then stop and report your current position and battery level.\\n-----------\\nresponse\\n-----------\\n[\\n\"import rclpy\",\\n\"from unitree_legged_msgs.msg import Position, BatteryStatus\",\\n\"from geometry_msgs.msg import Point\",\\n\"import time\",\\n\"rclpy.init()\",\\n\"node = rclpy.create_node(\\'walk_and_report\\')\",\\n\"pub_pos = node.create_publisher(Position, \\'/unitree_legged_control/position\\', 10)\",\\n\"pub_battery = node.create_publisher(BatteryStatus, \\'/unitree_legged_control/battery_status\\', 10)\",\\n\"point = Point()\",\\n\"point.x = 10\",\\n\"point.y = 0\",\\n\"msg_pos = Position()\",\\n\"msg_pos.pose.position = point\",\\n\"pub_pos.publish(msg_pos)\",\\n\"time.sleep(10)\",\\n\"point.x = 0\",\\n\"point.y = 0\",\\n\"msg_pos.pose.position = point\",\\n\"pub_pos.publish(msg_pos)\",\\n\"msg_battery = BatteryStatus()\",\\n\"pub_battery.publish(msg_battery)\",\\n\"rclpy.spin(node)\",\\n\"node.destroy_node()\",\\n\"rclpy.shutdown()\"\\n]\\n-----------', 'prompt\\n-----------\\nWalk backwards for 7 seconds, then stop and report your current position.\\n-----------\\nresponse\\n-----------\\n[\\n\"import rclpy\",\\n\"from unitree_legged_msgs.msg import Position\",\\n\"from geometry_msgs.msg import Point\",\\n\"import time\",\\n\"rclpy.init()\",\\n\"node = rclpy.create_node(\\'walk_back_and_report\\')\",\\n\"pub = node.create_publisher(Position, \\'/unitree_legged_control/position\\', 10)\",\\n\"point = Point()\",\\n\"point.x = -7\",\\n\"point.y = 0\",\\n\"msg = Position()\",\\n\"msg.pose.position = point\",\\n\"pub.publish(msg)\",\\n\"time.sleep(7)\",\\n\"point.x = 0\",\\n\"point.y = 0\",\\n\"msg.pose.position = point\",\\n\"pub.publish(msg)\",\\n\"rclpy.spin(node)\",\\n\"node.destroy_node()\",\\n\"rclpy.shutdown()\"\\n]\\n-----------']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from openai import OpenAI\n",
        "\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except Exception as e:\n",
        "    pass\n",
        "\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if api_key:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "else:\n",
        "    print(\"Warning: OPENAI_API_KEY not found in environment or .env\")\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "N_RETRIES = 3\n",
        "\n",
        "@retry(stop=stop_after_attempt(N_RETRIES), wait=wait_exponential(multiplier=1, min=4, max=70))\n",
        "def generate_example(prompt, prev_examples, temperature=.5):\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"You are generating data which will be used to train a machine learning model.\\n\\nYou will be given a high-level description of the model we want to train, and from that, you will generate data samples, each with a prompt/response pair.\\n\\nYou will do so in this format:\\n```\\nprompt\\n-----------\\n$prompt_goes_here\\n-----------\\n\\nresponse\\n-----------\\n$response_goes_here\\n-----------\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\n\\nHere is the type of model we want to train:\\n`{prompt}`\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    if len(prev_examples) > 0:\n",
        "        if len(prev_examples) > 8:\n",
        "            prev_examples = random.sample(prev_examples, 8)\n",
        "        for example in prev_examples:\n",
        "            messages.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": example\n",
        "            })\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=1000,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Generate examples\n",
        "prev_examples = []\n",
        "for i in range(number_of_examples):\n",
        "    print(f'Generating example {i}')\n",
        "    example = generate_example(prompt, prev_examples, temperature)\n",
        "    prev_examples.append(example)\n",
        "\n",
        "print(prev_examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC6iJzXjugJ-"
      },
      "source": [
        "We also need to generate a system message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMcfhW6Guh2E",
        "outputId": "54d7ccff-05bc-4574-b33b-fd3a99171bbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The system message is: `Given a high-level, natural language command, generate a sequence of Python code using ROS2 functions and Unitree SDK commands.`. Feel free to re-run this cell if you want a better result.\n"
          ]
        }
      ],
      "source": [
        "def generate_system_message(prompt):\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "          {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You will be given a high-level description of the model we are training, and from that, you will generate a simple system prompt for that model to use. Remember, you are not generating the system message for data generation -- you are generating the system message to use for inference. A good format to follow is `Given $INPUT_DATA, you will $WHAT_THE_MODEL_SHOULD_DO.`.\\n\\nMake it as concise as possible. Include nothing but the system prompt in your response.\\n\\nFor example, never write: `\\\"$SYSTEM_PROMPT_HERE\\\"`.\\n\\nIt should be like: `$SYSTEM_PROMPT_HERE`.\"\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": prompt.strip(),\n",
        "          }\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "        max_tokens=500,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "system_message = generate_system_message(prompt)\n",
        "\n",
        "print(f'The system message is: `{system_message}`. Feel free to re-run this cell if you want a better result.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6BqZ-hjseBF"
      },
      "source": [
        "Now let's put our examples into a dataframe and turn them into a final pair of datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CEdkYeRsdmB",
        "outputId": "d3d75852-6067-4cdf-b619-7442ba23dbb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 90 successfully-generated examples.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize lists to store prompts and responses\n",
        "prompts = []\n",
        "responses = []\n",
        "\n",
        "# Parse out prompts and responses from examples\n",
        "for example in prev_examples:\n",
        "  try:\n",
        "    split_example = example.split('-----------')\n",
        "    prompts.append(split_example[1].strip())\n",
        "    responses.append(split_example[3].strip())\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'prompt': prompts,\n",
        "    'response': responses\n",
        "})\n",
        "\n",
        "# Remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "print('There are ' + str(len(df)) + ' successfully-generated examples.')\n",
        "\n",
        "# Initialize list to store training examples\n",
        "training_examples = []\n",
        "\n",
        "# Create training examples in the format required for GPT-3.5 fine-tuning\n",
        "for index, row in df.iterrows():\n",
        "    training_example = {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_message.strip()},\n",
        "            {\"role\": \"user\", \"content\": row['prompt']},\n",
        "            {\"role\": \"assistant\", \"content\": row['response']}\n",
        "        ]\n",
        "    }\n",
        "    training_examples.append(training_example)\n",
        "\n",
        "# Save training examples to a .jsonl file\n",
        "with open('training_examples.jsonl', 'w') as f:\n",
        "    for example in training_examples:\n",
        "        f.write(json.dumps(example) + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWTY6qVgXD_T"
      },
      "source": [
        "# Upload the file to OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AWI7ZNpgVo8",
        "outputId": "29909fbc-0d8a-4039-fd9a-e3091a433784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… File uploaded successfully: file-BuTc4bZVxeddzfRJqLzkXw\n"
          ]
        }
      ],
      "source": [
        "with open(\"training_examples.jsonl\", \"rb\") as f:\n",
        "    uploaded_file = client.files.create(\n",
        "        file=f,\n",
        "        purpose=\"fine-tune\"\n",
        "    )\n",
        "\n",
        "file_id = uploaded_file.id\n",
        "print(\"âœ… File uploaded successfully:\", file_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmYRIq8dW9IR"
      },
      "source": [
        "# Train the model! You may need to wait a few minutes before running the next cell to allow for the file to process on OpenAI's servers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3a5HUtRghL7",
        "outputId": "2a2b7218-3746-4117-efa0-d90309dc7bc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸŽ¯ Fine-tuning job started: ftjob-SrtqdHtaxDBjEuOXNnj3AUqF\n"
          ]
        }
      ],
      "source": [
        "# âœ… Create fine-tuning job\n",
        "job = client.fine_tuning.jobs.create(\n",
        "    training_file=file_id,\n",
        "    model=\"gpt-3.5-turbo\"\n",
        ")\n",
        "\n",
        "job_id = job.id\n",
        "print(\"ðŸŽ¯ Fine-tuning job started:\", job_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUSX5QzmZMTd"
      },
      "source": [
        "# Now, just wait until the fine-tuning run is done, and you'll have a ready-to-use model!\n",
        "\n",
        "Run this cell every 20 minutes or so -- eventually, you'll see a message \"New fine-tuned model created: ft:gpt-3.5-turbo-0613:xxxxxxxxxxxx\"\n",
        "\n",
        "Once you see that message, you can go to the OpenAI Playground (or keep going to the next cells and use the API) to try the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNejHrsggwoe",
        "outputId": "c36abe84-2f57-4a2d-c3d1-6d4483e9d287"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 29/270: training loss=0.01\n",
            "Step 28/270: training loss=0.02\n",
            "Step 27/270: training loss=0.02\n",
            "Step 26/270: training loss=0.04\n",
            "Step 25/270: training loss=0.02\n",
            "Step 24/270: training loss=0.06\n",
            "Step 23/270: training loss=0.04\n",
            "Step 22/270: training loss=0.07\n",
            "Step 21/270: training loss=0.23\n",
            "Step 20/270: training loss=0.07\n"
          ]
        }
      ],
      "source": [
        "events = client.fine_tuning.jobs.list_events(\n",
        "    fine_tuning_job_id=job_id,\n",
        "    limit=10\n",
        ")\n",
        "\n",
        "for event in events.data:\n",
        "    print(event.message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91ihW2O27Phl"
      },
      "source": [
        "# Once your model is trained, run the next cell to grab the fine-tuned model name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0F-soCRg5Xi",
        "outputId": "f76dd9fd-2a4e-4a09-a9ea-901e014fe1f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Fine-tuned model name: None\n"
          ]
        }
      ],
      "source": [
        "job_info = client.fine_tuning.jobs.retrieve(job_id)\n",
        "\n",
        "# âœ… Get the fine-tuned model name\n",
        "model_name = job_info.fine_tuned_model\n",
        "\n",
        "print(\"âœ… Fine-tuned model name:\", model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n",
            "Fine-tuning job status: running\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msucceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcancelled\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# wait 10 seconds before checking again\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msucceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     12\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m job_info\u001b[38;5;241m.\u001b[39mfine_tuned_model\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "while True:\n",
        "    job_info = client.fine_tuning.jobs.retrieve(job_id)\n",
        "    status = job_info.status\n",
        "    print(\"Fine-tuning job status:\", status)\n",
        "    if status in [\"succeeded\", \"failed\", \"cancelled\"]:\n",
        "        break\n",
        "    time.sleep(10)  # wait 10 seconds before checking again\n",
        "\n",
        "if status == \"succeeded\":\n",
        "    model_name = job_info.fine_tuned_model\n",
        "    print(\"âœ… Fine-tuned model name:\", model_name)\n",
        "else:\n",
        "    print(\"âŒ Fine-tuning did not succeed. Status:\", status)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OmZLoBX7oQM"
      },
      "source": [
        "# Let's try it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "1YjU63EGhDpp",
        "outputId": "3e621bc3-23cb-439f-e0cf-19edc35e6a82"
      },
      "outputs": [
        {
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': 'you must provide a model parameter', 'type': 'invalid_request_error', 'param': None, 'code': None}}",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
            "File \u001b[1;32mc:\\Users\\kavee\\miniconda3\\envs\\gptgimhan\\lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\kavee\\miniconda3\\envs\\gptgimhan\\lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1156\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m   1112\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m not_given,\n\u001b[0;32m   1154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m   1155\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m-> 1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1167\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1168\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1169\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1170\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1172\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1173\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1174\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1179\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1184\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1186\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1187\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1188\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\kavee\\miniconda3\\envs\\gptgimhan\\lib\\site-packages\\openai\\_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1258\u001b[0m     )\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[1;32mc:\\Users\\kavee\\miniconda3\\envs\\gptgimhan\\lib\\site-packages\\openai\\_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'you must provide a model parameter', 'type': 'invalid_request_error', 'param': None, 'code': None}}"
          ]
        }
      ],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_message,\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": df['prompt'].sample().values[0],\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gptgimhan",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.24"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
